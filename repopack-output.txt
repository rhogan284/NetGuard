================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2024-09-19T02:50:54.607Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
db/
  init.sql
locust/
  locustfile.py
  threat_locustfile.py
logstash/
  logstash.conf
  logstash.yaml
threat_detector/
  config.yaml
  Dockerfile
  Dockerfile.responder
  requirements.txt
  responder_config.yaml
  threat_detector.py
  threat_responder.py
web/
  app.py
  Dockerfile
  requirements.txt
docker-compose.yml

================================================================
Repository Files
================================================================

================
File: db/init.sql
================
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    price DECIMAL(10, 2) NOT NULL
);

INSERT INTO products (name, price) VALUES
    ('Laptop', 999.99),
    ('Smartphone', 599.99),
    ('Headphones', 149.99);

================
File: locust/locustfile.py
================
import random
import json
import logging
import time
import uuid
from locust import HttpUser, task, between
from locust.contrib.fasthttp import FastHttpUser
from datetime import datetime

json_logger = logging.getLogger('json_logger')
json_logger.setLevel(logging.INFO)
json_handler = logging.FileHandler('/mnt/logs/locust_json.log')
json_handler.setFormatter(logging.Formatter('%(message)s'))
json_logger.addHandler(json_handler)

class WebsiteUser(FastHttpUser):
    wait_time = between(1, 5)

    def on_start(self):
        self.user_id = str(uuid.uuid4())
        self.session_id = str(uuid.uuid4())
        self.client_ip = f"{random.randint(1, 223)}.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(1, 254)}"
        self.username = random.choice([
            'applebee',
            'ofgirl',
            'bigbuffmen',
            'alphagamer101',
            'donaldtrump'
        ])
        self.password = random.choice([
            'password', 
            '123456', 
            'admin', 
            'qwerty', 
            'letmein'
        ])
        self.user_agent = random.choice([
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (Linux; Android 11; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36'
        ])
        self.geolocation = random.choice([
            {"country": "United States", "city": "New York", "timezone": "America/New_York"},
            {"country": "United Kingdom", "city": "London", "timezone": "Europe/London"},
            {"country": "Japan", "city": "Tokyo", "timezone": "Asia/Tokyo"},
            {"country": "Australia", "city": "Sydney", "timezone": "Australia/Sydney"},
            {"country": "Germany", "city": "Berlin", "timezone": "Europe/Berlin"}
        ])

    @task(10)
    def index_page(self):
        self._log_request("GET", "/", None)

    @task(5)
    def view_product(self):
        product_id = random.randint(1, 10)
        self._log_request("GET", f"/products/{product_id}", None)

    @task(2)
    def add_to_cart(self):
        product_id = random.randint(1, 10)
        self._log_request("POST", "/cart", {"product_id": product_id, "quantity": 1})

    @task(2)
    def view_cart(self):
        self._log_request("GET", "/cart", None)

    @task(1)
    def checkout(self):
        self._log_request("POST", "/checkout", {"payment_method": "credit_card"})

    @task(1)
    def login(self):
        self._log_request("POST", "/login", {"username": self.username, "password": self.password})

    def _log_request(self, method, path, data):
        log_id = str(uuid.uuid4())
        start_time = time.time()
        try:
            if method == "GET":
                response = self.client.get(path)
            elif method == "POST":
                response = self.client.post(path, json=data)
            else:
                raise ValueError(f"Unsupported HTTP method: {method}")

            self._log_response(log_id, method, path, response, start_time, data)
        except Exception as e:
            self._log_exception(log_id, method, path, e, start_time, data)

    def _log_response(self, log_id, method, path, response, start_time, data):
        log_entry = {
            "log_id": log_id,
            "@timestamp": datetime.utcnow().isoformat(),
            "client_ip": self.client_ip,
            "method": method,
            "url": f"{self.host}{path}",
            "status_code": response.status_code,
            "response_time_ms": int((time.time() - start_time) * 1000),
            "bytes_sent": len(response.request.body) if response.request.body else 0,
            "bytes_received": len(response.content),
            "user_agent": self.user_agent,
            "referer": random.choice([None, "https://www.google.com", "https://www.bing.com"]),
            "request_headers": dict(response.request.headers),
            "response_headers": dict(response.headers),
            "geo": self.geolocation,
            "request_body": data if data else None
        }
        json_logger.info(json.dumps(log_entry))

    def _log_exception(self, log_id, method, path, exception, start_time, data):
        log_entry = {
            "log_id": log_id,
            "@timestamp": datetime.utcnow().isoformat(),
            "client_ip": self.client_ip,
            "method": method,
            "url": f"{self.host}{path}",
            "status_code": 500,
            "response_time_ms": int((time.time() - start_time) * 1000),
            "exception": str(exception),
            "user_agent": self.user_agent,
            "referer": random.choice([None, "https://www.google.com", "https://www.bing.com", "https://example.com"]),
            "geo": self.geolocation,
            "request_body": data if data else None
        }
        json_logger.info(json.dumps(log_entry))

================
File: locust/threat_locustfile.py
================
import random
import json
import logging
import time
import uuid
from locust import HttpUser, task, between
from locust.contrib.fasthttp import FastHttpUser
from datetime import datetime

json_logger = logging.getLogger('json_logger')
json_logger.setLevel(logging.INFO)
json_handler = logging.FileHandler('/mnt/logs/threat_locust_json.log')
json_handler.setFormatter(logging.Formatter('%(message)s'))
json_logger.addHandler(json_handler)

class MaliciousUser(FastHttpUser):
    wait_time = between(1, 10)

    def randomuser(self):
        self.user_id = str(uuid.uuid4())
        self.session_id = str(uuid.uuid4())
        self.client_ip = f"{random.randint(1, 223)}.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(1, 254)}"
        self.user_agent = random.choice([
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
            'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)',
            'sqlmap/1.4.7#stable (http://sqlmap.org)',
            'Nikto/2.1.6',
            'Acunetix-WebVulnerability-Scanner/1.0',
        ])
        self.geolocation = random.choice([
            {"country": "Russia", "city": "Moscow", "timezone": "Europe/Moscow"},
            {"country": "China", "city": "Beijing", "timezone": "Asia/Shanghai"},
            {"country": "United States", "city": "Ashburn", "timezone": "America/New_York"},
            {"country": "Netherlands", "city": "Amsterdam", "timezone": "Europe/Amsterdam"},
        ])
        return self

    def on_start(self):
        self.randomuser()

    def get_headers(self):
        return {
            'X-Forwarded-For': self.client_ip,
            'User-Agent': self.user_agent
        }

    @task(2)
    def sql_injection_attempt(self):
        payloads = [
            "' OR '1'='1",
            "' UNION SELECT username, password FROM users--",
            "admin'--",
            "1; DROP TABLE users--",
            "' OR 1=1--",
            "' UNION SELECT null, version()--",
            "' AND 1=2 UNION SELECT null, null--",
            "' OR 'x'='x'--",
            "1; EXEC xp_cmdshell('ping 127.0.0.1')--",
            "<script>alert('XSS')</script>",
             "<img src=x onerror=alert('XSS')>",
             "../../../../etc/passwd",
             "../../../../etc/passwd%00",
             "php://filter/convert.base64-encode/resource=index.php",
             "http://malicious-website.com/malicious-script.php",
              "1; ls -la",
              "1 && whoami",
        ]
        payload = random.choice(payloads)
        self._log_request("GET", f"/products?id={payload}", None, "sql_injection")

    @task(2)
    def xss_attempt(self):
        payloads = [
            "<script>alert('XSS')</script>",
            "<img src=x onerror=alert('XSS')>",
            "javascript:alert('XSS')",
            "<svg onload=alert('XSS')>",
            "'\"><script>alert('XSS')</script>",
        ]
        payload = random.choice(payloads)
        self._log_request("POST", "/search", {"q": payload}, "xss")

    # @task(2)
    # def brute_force_login(self):
    #     usernames = ['admin', 'root', 'user', 'test', 'guest', 'applebee', 'ofgirl', 'bigbuffmen', 'alphagamer101', 'donaldtrump']
    #     passwords = ['password', '123456', 'admin', 'qwerty', 'letmein', 'nonosquare']
    #
    #     for username in usernames:
    #         for password in passwords:
    #             self.setrandom()
    #             self._log_request("POST", "/login", {"username": username, "password": password}, "brute_force")

    @task(1)
    def path_traversal_attempt(self):
        payloads = [
            "../../../etc/passwd",
            "..\\..\\..\\windows\\win.ini",
            "....//....//....//etc/hosts",
            "%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd",
            "..%252f..%252f..%252fetc%252fpasswd",
        ]
        payload = random.choice(payloads)
        self._log_request("GET", f"/static/{payload}", None, "path_traversal")

    @task(1)
    def command_injection_attempt(self):
        payloads = [
            "; cat /etc/passwd",
            "& ipconfig",
            "| ls -la",
            "`whoami`",
            "$(echo 'vulnerable')",
        ]
        payload = random.choice(payloads)
        self._log_request("GET", f"/exec?cmd=date{payload}", None, "command_injection")

    # @task(2)
    # def web_scraping(self):
    #     pages = ["/products", "/categories", "/reviews", "/comments", "/carts", '/information', '/aboutus']
    #     for page in pages:
    #         self._log_request("GET", page, None, "web_scraping")

    # @task(2)
    # def ddos_simulation(self):
    #
    #     randomuser = random.randint(1,2)
    #     for _ in range(random.randint(5, 15)):
    #         # Randomize user_id, session_id, client_ip, and user_agent
    #         if randomuser == 1:
    #             self.randomuser()
    #
    #         actions = [
    #             lambda: self._log_request("GET", "/", None, "ddos"),
    #             lambda: self._log_request("GET", f"/products/{random.randint(1, 10)}", None, "ddos"),
    #             lambda: self._log_request("POST", "/cart", {"product_id": random.randint(1, 10), "quantity": 1}, "ddos"),
    #             lambda: self._log_request("GET", "/cart", None, "ddos"),
    #             lambda: self._log_request("POST", "/checkout", {"payment_method": "credit_card"}, "ddos")
    #         ]
    #
    #         for _ in range(random.randint(1, 20)):
    #             random.choice(actions)()

    def _log_request(self, method, path, data, threat_type):
        log_id = str(uuid.uuid4())
        start_time = time.time()
        headers = self.get_headers()
        try:
            if method == "GET":
                response = self.client.get(path, headers=headers)
            elif method == "POST":
                response = self.client.post(path, json=data, headers=headers)
            else:
                raise ValueError(f"Unsupported HTTP method: {method}")

            self._log_response(log_id, method, path, response, start_time, data, threat_type)
        except Exception as e:
            self._log_exception(log_id, method, path, e, start_time, data, threat_type)

    def _log_response(self, log_id, method, path, response, start_time, data, threat_type):
        log_entry = {
            "log_id": log_id,
            "threat_type": threat_type,
            "@timestamp": datetime.utcnow().isoformat(),
            "client_ip": self.client_ip,
            "method": method,
            "url": f"{self.host}{path}",
            "status_code": response.status_code,
            "response_time_ms": int((time.time() - start_time) * 1000),
            "bytes_sent": len(response.request.body) if response.request.body else 0,
            "bytes_received": len(response.content),
            "user_agent": self.user_agent,
            "referer": random.choice([None, "https://www.google.com", "https://www.bing.com", "https://example.com"]),
            "request_headers": self.get_headers(),
            "response_headers": dict(response.headers),
            "geo": self.geolocation,
            "request_body": data if data else None,
        }
        json_logger.info(json.dumps(log_entry))

    def _log_exception(self, log_id, method, path, exception, start_time, data, threat_type):
        log_entry = {
            "log_id": log_id,
            "threat_type": threat_type,
            "@timestamp": datetime.utcnow().isoformat(),
            "client_ip": self.client_ip,
            "method": method,
            "url": f"{self.host}{path}",
            "status_code": 500,
            "response_time_ms": int((time.time() - start_time) * 1000),
            "exception": str(exception),
            "user_agent": self.user_agent,
            "referer": random.choice([None, "https://www.google.com", "https://www.bing.com", "https://example.com"]),
            "geo": self.geolocation,
            "request_body": data if data else None,
        }
        json_logger.info(json.dumps(log_entry))

================
File: logstash/logstash.conf
================
input {
  file {
    path => "/mnt/logs/locust_json.log"
    start_position => "beginning"
    codec => json
    type => "normal"
  }
  file {
    path => "/mnt/logs/threat_locust_json.log"
    start_position => "beginning"
    codec => json
    type => "threat"
  }
}

filter {
  date {
    match => [ "@timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSS" ]
    target => "@timestamp"
  }

  mutate {
    convert => {
      "bytes_received" => "integer"
      "bytes_sent" => "integer"
      "response_time_ms" => "integer"
      "status_code" => "integer"
    }
    remove_field => [ "host" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "locust-logs-%{+YYYY.MM.dd}"
  }
  stdout { codec => rubydebug }
}

================
File: logstash/logstash.yaml
================
log.level: warn

================
File: threat_detector/config.yaml
================
# Index names
indices:
  source: locust-logs-*
  threat: threat-logs
  normal: normal-logs

# DDoS detection settings
ddos:
  threshold: 5
  time_window: 2
  max_requests: 1000

# Threat detection rules
detection_rules:
  sql_injection:
    - |-
      id=\s*['"].*?(?:--|\%27|')
    - |-
      UNION\s+SELECT
    - |-
      EXEC\s*\(
    - |-
      WAITFOR\s+DELAY
    - |-
      SELECT\s+.*?FROM
    - |-
      1\s*=\s*1
    - |-
      DROP\s+TABLE
    - |-
      ;.*?(?:SELECT|INSERT|UPDATE|DELETE|DROP)
  xss:
    - "<script>"
    - "javascript:"
    - "alert\\s*\\("
    - "on\\w+\\s*="
    - "<svg.*?on\\w+\\s*="
    - "<img.*?on\\w+\\s*="
    - "\"\\s*><script>"
    - "'\\s*><script>"
  path_traversal:
    - "\\.\\.\\/|\\.\\.\\\\"
    - "\\.\\.(\\%2f|\\%5c)"
    - "\\%2e\\%2e(\\%2f|\\%5c)"
    - "\\%252e\\%252e(\\%252f|\\%255c)"
  command_injection:
    - ";\\s*\\w+"
    - "`.*?`"
    - "\\|\\s*\\w+"
    - "\\$\\(.*?\\)"
    - "&&\\s*\\w+"
    - "\\|\\|\\s*\\w+"

# Log processing settings
processing:
  batch_size: 1000
  poll_interval: 5
  error_retry_interval: 30

# Logging configuration
logging:
  level: INFO
  file: /mnt/logs/threat_detector.log
  max_size: 10485760  # 10 MB
  backup_count: 5

# Field order for log entries
field_order:
  - log_id
  - threat_type
  - detected_threats
  - "@timestamp"
  - client_ip
  - method
  - url
  - status_code
  - response_time_ms
  - bytes_sent
  - bytes_received
  - user_agent
  - referer
  - request_headers
  - response_headers
  - geo
  - request_body

================
File: threat_detector/Dockerfile
================
FROM python:3.12
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY threat_detector.py .
COPY config.yaml /app/config.yaml

# Create the logs directory
RUN mkdir -p /mnt/logs && chmod 777 /mnt/logs


CMD ["python", "threat_detector.py"]

================
File: threat_detector/Dockerfile.responder
================
FROM python:3.12

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY threat_responder.py .
COPY responder_config.yaml .

CMD ["python", "threat_responder.py"]

================
File: threat_detector/requirements.txt
================
elasticsearch==8.9.0
PyYAML==6.0.2
watchdog==3.0.0
redis==5.0.1

================
File: threat_detector/responder_config.yaml
================
# Logging configuration
logging:
  level: INFO
  file: /mnt/logs/threat_responder.log
  max_size: 10485760  # 10 MB
  backup_count: 5

# Response actions for different threat types
response_actions:
  sql_injection: "block_ip"
  xss: "block_ip"
  path_traversal: "log"
  command_injection: "block_ip"
  ddos: "block_ip"
  potential_ddos: "rate_limit"
  potential_brute_force: "rate_limit"

# Redis configuration
redis:
  key_prefix: "threat_responder:"
  blocked_ips_key: "blocked_ips"
  expiration_time: 3600

# Rate limiting configuration
rate_limit:
  window_size: 60
  max_requests: 100

elasticsearch:
  host: elasticsearch
  port: 9200

# Index names
indices:
  threat: threat-logs

# Processing settings
processing:
  batch_size: 100
  poll_interval: 5
  error_retry_interval: 30

sync_interval: 300

================
File: threat_detector/threat_detector.py
================
import json
import re
from collections import defaultdict, deque
from datetime import datetime, timezone, timedelta
import os
import logging
import yaml
import time
import uuid
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

log_dir = '/mnt/logs'
os.makedirs(log_dir, exist_ok=True)

threat_logger = logging.getLogger('threat_logger')
threat_logger.setLevel(logging.WARNING)
file_handler = logging.FileHandler(os.path.join(log_dir, 'detected_threats.log'))
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
threat_logger.addHandler(file_handler)

threat_logger.propagate = False

class ThreatDetector:
    def __init__(self, config_path='config.yaml'):
        self.config = self.load_config(config_path)
        self.es = self.connect_to_elasticsearch()
        self.compiled_rules = self.compile_rules()
        self.request_timestamps = defaultdict(lambda: deque(maxlen=self.config['ddos']['max_requests']))
        self.last_processed_timestamp = self.get_last_processed_timestamp()

    @staticmethod
    def load_config(config_path):
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)

    def connect_to_elasticsearch(self):
        es_host = os.environ.get('ELASTICSEARCH_HOST', 'elasticsearch')
        es_port = os.environ.get('ELASTICSEARCH_PORT', '9200')
        es = Elasticsearch([f"http://{es_host}:{es_port}"])
        es.info()
        logger.info("Successfully connected to Elasticsearch")
        return es

    def get_last_processed_timestamp(self):
        try:
            with open('/mnt/logs/last_processed_timestamp.txt', 'r') as f:
                return datetime.fromisoformat(f.read().strip())
        except FileNotFoundError:
            return datetime.now(timezone.utc) - timedelta(minutes=5)

    def save_last_processed_timestamp(self, timestamp):
        with open('/mnt/logs/last_processed_timestamp.txt', 'w') as f:
            f.write(timestamp.isoformat())

    def compile_rules(self):
        return {
            threat_type: [re.compile(pattern, re.IGNORECASE) for pattern in patterns]
            for threat_type, patterns in self.config['detection_rules'].items()
        }

    def detect_threats(self, log_entry):
        threats = set()
        url = log_entry.get('url', '')
        method = log_entry.get('method', '')
        request_body = json.dumps(log_entry.get('request_body', {}))
        headers = json.dumps(log_entry.get('request_headers', {}))
        client_ip = log_entry.get('client_ip', '')
        timestamp = datetime.now(timezone.utc)

        content_to_check = f"{url} {request_body} {headers}"

        for threat_type, patterns in self.compiled_rules.items():
            if threat_type != "ddos" and any(pattern.search(content_to_check) for pattern in patterns):
                threats.add(threat_type)

        if method == 'POST' and '/login' in url:
            threats.add('potential_brute_force')

        if '/exec' in url and 'cmd' in url:
            threats.add('command_injection')

        # DDoS detection
        self.request_timestamps[client_ip].append(timestamp)

        self.request_timestamps[client_ip] = deque(
            filter(lambda ts: timestamp - ts <= timedelta(seconds=self.config['ddos']['time_window']),
                   self.request_timestamps[client_ip]),
            maxlen=self.config['ddos']['max_requests']
        )

        if len(self.request_timestamps[client_ip]) > self.config['ddos']['threshold']:
            threats.add('ddos')
        elif len(self.request_timestamps[client_ip]) > 1:
            threats.add('potential_ddos')

        return list(threats)

    def reorder_log_fields(self, log_entry):
        ordered_log = {}
        for field in self.config['field_order']:
            if field in log_entry:
                ordered_log[field] = log_entry[field]
            elif field == "log_id":
                ordered_log[field] = str(uuid.uuid4())
            elif field == "threat_type":
                ordered_log[field] = log_entry.get("type", "unknown")

        for key, value in log_entry.items():
            if key not in ordered_log:
                ordered_log[key] = value

        return ordered_log

    def process_logs_batch(self, logs):
        actions = []
        for log in logs:
            threats = self.detect_threats(log['_source'])
            reordered_log = self.reorder_log_fields(log['_source'])
            if threats:
                reordered_log['detected_threats'] = threats
                actions.append({
                    "_index": self.config['indices']['threat'],
                    "_source": reordered_log
                })
                threat_message = f"Threat detected: {threats} in log: {reordered_log.get('url', 'N/A')} from IP: {reordered_log.get('client_ip', 'N/A')}"
                logging.warning(threat_message)
                threat_logger.warning(json.dumps(reordered_log))
            else:
                actions.append({
                    "_index": self.config['indices']['normal'],
                    "_source": reordered_log
                })
                logging.info(
                    f"Normal log processed: {reordered_log.get('url', 'N/A')} from IP: {reordered_log.get('client_ip', 'N/A')}")

        if actions:
            try:
                success, failed = bulk(self.es, actions)
                logger.info(f"Indexed {success} logs. Failed: {len(failed)}")
            except Exception as e:
                logger.error(f"Error during bulk indexing: {str(e)}")

    def get_new_logs(self):
        query = {
            "bool": {
                "must": [
                    {
                        "range": {
                            "@timestamp": {
                                "gt": self.last_processed_timestamp.isoformat()
                            }
                        }
                    }
                ]
            }
        }

        result = self.es.search(index=self.config['indices']['source'], query=query, sort=[{"@timestamp": "asc"}],
                                size=self.config['processing']['batch_size'])
        logging.info(f"Retrieved {len(result['hits']['hits'])} new logs from Elasticsearch")
        return result['hits']['hits']

    def run(self):
        while True:
            try:
                logs = self.get_new_logs()
                if logs:
                    self.process_logs_batch(logs)
                    last_log = logs[-1]['_source']
                    self.last_processed_timestamp = datetime.fromisoformat(last_log['@timestamp'].replace('Z', '+00:00'))
                    self.save_last_processed_timestamp(self.last_processed_timestamp)
                    logger.info(f"Processed {len(logs)} logs. Last processed timestamp: {self.last_processed_timestamp.isoformat()}")
                else:
                    logger.info("No new logs to process.")
                time.sleep(self.config['processing']['poll_interval'])
            except Exception as e:
                logger.error(f"An error occurred: {str(e)}")
                logger.info("Attempting to reconnect to Elasticsearch...")
                self.es = self.connect_to_elasticsearch()
                time.sleep(self.config['processing']['error_retry_interval'])


if __name__ == "__main__":
    detector = ThreatDetector()
    detector.run()

================
File: threat_detector/threat_responder.py
================
import json
import logging
import yaml
import time
import redis
import os
from datetime import datetime, timedelta
from elasticsearch import Elasticsearch
import sys

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[
                        logging.StreamHandler(sys.stderr),
                        logging.FileHandler("/mnt/logs/threat_responder.log")
                    ])
logger = logging.getLogger(__name__)

class ThreatResponder:
    def __init__(self, config_path='responder_config.yaml'):
        logger.info("Initializing ThreatResponder")
        self.config = self.load_config(config_path)
        self.es = self.connect_to_elasticsearch()
        self.redis = self.connect_to_redis()
        self.last_processed_timestamp = self.get_last_processed_timestamp()
        self.BLOCKED_IPS_KEY = f"{self.config['redis']['key_prefix']}blocked_ips"

    @staticmethod
    def load_config(config_path):
        logger.info(f"Loading config from {config_path}")
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        logger.info("Config loaded successfully")
        return config

    def connect_to_elasticsearch(self):
        es_host = os.environ.get('ELASTICSEARCH_HOST', 'elasticsearch')
        es_port = os.environ.get('ELASTICSEARCH_PORT', '9200')
        es = Elasticsearch([f"http://{es_host}:{es_port}"])
        es.info()
        logger.info("Successfully connected to Elasticsearch")
        return es

    def connect_to_redis(self):
        redis_url = os.environ.get('REDIS_URL', 'redis://redis:6379/0')
        logger.info(f"Connecting to Redis at {redis_url}")
        return redis.Redis.from_url(redis_url, decode_responses=True)

    def get_last_processed_timestamp(self):
        try:
            timestamp = self.redis.get('last_processed_timestamp')
            if timestamp:
                return datetime.fromisoformat(timestamp)
        except Exception as e:
            logger.error(f"Error getting last processed timestamp: {str(e)}")

        return datetime.utcnow() - timedelta(minutes=5)

    def save_last_processed_timestamp(self, timestamp):
        try:
            self.redis.set('last_processed_timestamp', timestamp.isoformat())
        except Exception as e:
            logger.error(f"Error saving last processed timestamp: {str(e)}")

    def get_new_threats(self):
        query = {
            "bool": {
                "must": [
                    {
                        "range": {
                            "@timestamp": {
                                "gt": self.last_processed_timestamp.isoformat()
                            }
                        }
                    }
                ]
            }
        }

        logger.info(f"Querying Elasticsearch for threats after {self.last_processed_timestamp.isoformat()}")
        logger.debug(f"Query: {json.dumps(query)}")

        result = self.es.search(
            index=self.config['indices']['threat'],
            query=query,
            sort=[{"@timestamp": "asc"}],
            size=self.config['processing']['batch_size']
        )

        threats = result['hits']['hits']
        logger.info(f"Retrieved {len(threats)} threats from Elasticsearch")

        if threats:
            logger.info(f"First threat timestamp: {threats[0]['_source']['@timestamp']}")
            logger.info(f"Last threat timestamp: {threats[-1]['_source']['@timestamp']}")

        return threats

    def execute_response(self, threat_type, log_entry):
        logger.info(f"Executing response for threat type: {threat_type}")
        if threat_type in self.config['response_actions']:
            action = self.config['response_actions'][threat_type]
            logger.info(f"Action for {threat_type}: {action}")

            client_ip = log_entry.get('client_ip', 'unknown')

            if action == "block_ip":
                self.block_ip(client_ip)
            elif action == "rate_limit":
                self.rate_limit_ip(client_ip)
            elif action == "log":
                self.log_threat(threat_type, client_ip)
            else:
                logger.warning(f"Unknown action type for threat: {threat_type}")
        else:
            logger.warning(f"No response action defined for threat type: {threat_type}")

    def block_ip(self, ip):
        logger.info(f"Blocking IP: {ip}")
        self.redis.sadd(self.BLOCKED_IPS_KEY, ip)
        self.redis.expire(self.BLOCKED_IPS_KEY, self.config['redis']['expiration_time'])
        logger.info(f"Blocked IP: {ip} for {self.config['redis']['expiration_time']} seconds")

    def rate_limit_ip(self, ip):
        logger.info(f"Rate limiting IP: {ip}")
        key = f"{self.config['redis']['key_prefix']}rate:{ip}"
        current = self.redis.get(key)
        if current is None:
            self.redis.set(key, 1, ex=self.config['rate_limit']['window_size'])
        elif int(current) < self.config['rate_limit']['max_requests']:
            self.redis.incr(key)
        else:
            self.block_ip(ip)
            logger.info(f"Rate limit exceeded for IP: {ip}. Blocking.")

    def log_threat(self, threat_type, ip):
        logger.info(f"Logging threat: {threat_type} from IP: {ip}")
        with open(self.config['logging']['file'], "a") as f:
            f.write(f"{datetime.now().isoformat()},{threat_type},{ip}\n")
        logger.info(f"Logged {threat_type} threat from IP: {ip}")

    def process_threats(self, threats):
        for threat in threats:
            log_entry = threat['_source']
            detected_threats = log_entry.get('detected_threats', [])
            client_ip = log_entry.get('client_ip')
            if client_ip and detected_threats:
                self.block_ip(client_ip)
                logger.info(f"Blocked IP {client_ip} due to threats: {', '.join(detected_threats)}")

        if threats:
            last_threat = threats[-1]['_source']
            self.last_processed_timestamp = datetime.fromisoformat(last_threat['@timestamp'].replace('Z', '+00:00'))
            self.save_last_processed_timestamp(self.last_processed_timestamp)

    def run(self):
        sync_interval = self.config.get('sync_interval', 300)  # Default to 5 minutes
        last_sync_time = time.time()

        while True:
            try:
                current_time = time.time()
                if current_time - last_sync_time > sync_interval:
                    self.sync_last_processed_timestamp()
                    last_sync_time = current_time

                threats = self.get_new_threats()
                if threats:
                    self.process_threats(threats)
                    logger.info(
                        f"Processed {len(threats)} threats. Last processed timestamp: {self.last_processed_timestamp.isoformat()}")
                else:
                    logger.info("No new threats to process.")
                time.sleep(self.config['processing']['poll_interval'])
            except Exception as e:
                logger.error(f"An error occurred: {str(e)}")
                logger.info("Attempting to reconnect to Elasticsearch...")
                self.es = self.connect_to_elasticsearch()
                time.sleep(self.config['processing']['error_retry_interval'])

    def sync_last_processed_timestamp(self):
        query = {
            "bool": {
                "must": [
                    {
                        "range": {
                            "@timestamp": {
                                "lte": datetime.utcnow().isoformat()
                            }
                        }
                    }
                ]
            }
        }

        result = self.es.search(
            index=self.config['indices']['threat'],
            query=query,
            sort=[{"@timestamp": "desc"}],
            size=1
        )

        if result['hits']['hits']:
            latest_timestamp = datetime.fromisoformat(
                result['hits']['hits'][0]['_source']['@timestamp'].replace('Z', '+00:00'))
            self.last_processed_timestamp = latest_timestamp
            self.save_last_processed_timestamp(latest_timestamp)
            logger.info(f"Synced last processed timestamp to {latest_timestamp.isoformat()}")
        else:
            logger.warning("No threats found in Elasticsearch during sync")


if __name__ == "__main__":
    responder = ThreatResponder()
    responder.run()

================
File: web/app.py
================
from flask import Flask, request, jsonify
import psycopg2
import os
import redis
import logging
from functools import lru_cache
import time

app = Flask(__name__)

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

REDIS_URL = os.environ.get('REDIS_URL', 'redis://redis:6379/0')
DATABASE_URL = os.environ.get('DATABASE_URL', 'postgresql://user:password@db:5432/ecommerce')
BLOCKED_IPS_KEY = "threat_responder:blocked_ips"

def get_redis_client():
    try:
        client = redis.Redis.from_url(REDIS_URL, decode_responses=True)
        client.ping()
        logger.info(f"Successfully connected to Redis at {REDIS_URL}")
        logger.info(f"Using blocked IPs key: {BLOCKED_IPS_KEY}")
        return client
    except redis.exceptions.ConnectionError as e:
        logger.error(f"Failed to connect to Redis: {e}")
        return None


redis_client = get_redis_client()

def get_db_connection():
    conn = psycopg2.connect(DATABASE_URL)
    conn.autocommit = True
    return conn

def is_ip_blocked(ip):
    try:
        is_blocked = redis_client.sismember(BLOCKED_IPS_KEY, ip)
        logger.info(f"Checking if IP {ip} is blocked. Result: {is_blocked}")
        all_members = redis_client.smembers(BLOCKED_IPS_KEY)
        logger.info(f"All blocked IPs: {all_members}")
        return is_blocked
    except redis.exceptions.RedisError as e:
        logger.error(f"Error checking if IP is blocked: {e}")
        return False

def block_ip(ip):
    try:
        redis_client.sadd(BLOCKED_IPS_KEY, ip)
        redis_client.expire(BLOCKED_IPS_KEY, 3600)
        logger.info(f"Blocked IP: {ip}")
    except redis.exceptions.RedisError as e:
        logger.error(f"Error blocking IP: {e}")

def get_client_ip():
    x_forwarded_for = request.headers.get('X-Forwarded-For')
    if x_forwarded_for:
        ip = x_forwarded_for.split(',')[0].strip()
    else:
        ip = request.remote_addr
    return ip

@app.before_request
def check_if_blocked():
    client_ip = get_client_ip()
    logger.info(f"Received request from IP: {client_ip}")
    logger.info(f"Headers: {request.headers}")
    if is_ip_blocked(client_ip):
        logger.warning(f"Blocked request from IP: {client_ip}")
        return jsonify({"error": "Access denied"}), 403
    logger.info(f"Allowed request from IP: {client_ip}")

@app.route('/')
def hello():
    return "Welcome to the E-commerce Platform Simulation!"


@app.route('/products')
def get_products():
    conn = get_db_connection()
    if conn is None:
        return jsonify({"error": "Database connection error"}), 500
    try:
        with conn.cursor() as cur:
            cur.execute('SELECT * FROM products;')
            products = cur.fetchall()
        return jsonify([{'id': p[0], 'name': p[1], 'price': p[2]} for p in products])
    except psycopg2.Error as e:
        logger.error(f"Database error: {e}")
        return jsonify({"error": "Database error"}), 500
    finally:
        conn.close()


@app.route('/products/<int:product_id>')
def get_product(product_id):
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute('SELECT * FROM products WHERE id = %s;', (product_id,))
    product = cur.fetchone()
    cur.close()
    conn.close()
    if product:
        return jsonify({'id': product[0], 'name': product[1], 'price': product[2]})
    return jsonify({"error": "Product not found"}), 404


@app.route('/login', methods=['POST'])
def login():
    return jsonify({"message": "Login simulation successful"})


@app.route('/cart', methods=['GET', 'POST'])
def cart():
    if request.method == 'POST':
        return jsonify({"message": "Item added to cart"})
    else:
        return jsonify({"message": "Cart viewed"})


@app.route('/checkout', methods=['POST'])
def checkout():
    return jsonify({"message": "Checkout process completed"})


@app.route('/search')
def search():
    query = request.args.get('q', '')
    return jsonify({"message": f"Search results for: {query}"})


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)

================
File: web/Dockerfile
================
FROM python:3.12
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY app.py .
ENV REDIS_KEY_PREFIX="threat_responder:"
CMD ["python", "app.py"]

================
File: web/requirements.txt
================
Flask==3.0.3
psycopg2-binary==2.9.9
redis==5.0.1

================
File: docker-compose.yml
================
services:
  db:
    image: postgres:13
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=ecommerce
    volumes:
      - ./db/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - app-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d ecommerce"]
      interval: 5s
      timeout: 5s
      retries: 5

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.0
    environment:
      - discovery.type=single-node
      - logger.level=WARN
    ports:
      - "9200:9200"
    networks:
      - app-network
    healthcheck:
      test: [ "CMD-SHELL", "curl -s http://localhost:9200 >/dev/null || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5

  logstash:
    image: docker.elastic.co/logstash/logstash:7.14.0
    volumes:
      - ./logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./logstash/logstash.yaml:/usr/share/logstash/config/logstash.yaml
      - ./logs:/mnt/logs
    depends_on:
      - elasticsearch
    ports:
      - "5044:5044"
    networks:
      - app-network

  kibana:
    image: docker.elastic.co/kibana/kibana:7.14.0
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - LOGGING_ROOT_LEVEL=warn
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - app-network

  threat-detector:
    build: ./threat_detector
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
    volumes:
      - ./logs:/mnt/logs
    restart: unless-stopped
    networks:
      - app-network

  redis:
    image: redis:7.4-alpine
    ports:
      - "6379:6379"
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50

  web:
    build: ./web
    ports:
      - "5002:5000"
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_started
    environment:
      - DATABASE_URL=postgresql://user:password@db:5432/ecommerce
      - REDIS_URL=redis://redis:6379/0
      - REDIS_KEY_PREFIX="threat_responder:"
    networks:
      - app-network
    restart: unless-stopped

  locust:
    image: locustio/locust
    ports:
      - "8089:8089"
    volumes:
      - ./locust:/mnt/locust
      - ./logs:/mnt/logs
    command: -f /mnt/locust/locustfile.py --host http://web:5000 --logfile /mnt/logs/locust.log --loglevel INFO
    depends_on:
      - web
    networks:
      - app-network

  threat-locust:
    image: locustio/locust
    ports:
      - "8090:8089"
    volumes:
      - ./locust:/mnt/locust
      - ./logs:/mnt/logs
    command: -f /mnt/locust/threat_locustfile.py --host http://web:5000 --logfile /mnt/logs/threat_locust.log --loglevel INFO
    depends_on:
      - web
    networks:
      - app-network

  threat-responder:
    build:
      context: ./threat_detector
      dockerfile: Dockerfile.responder
    volumes:
      - ./logs:/mnt/logs
      - ./threat_detector/responder_config.yaml:/app/responder_config.yaml
    depends_on:
      - threat-detector
      - redis
    environment:
      - REDIS_URL=redis://redis:6379/0
      - REDIS_KEY_PREFIX="threat_responder:"
    networks:
      - app-network
    restart: unless-stopped

networks:
  app-network:
    driver: bridge

volumes:
  logs:
